{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.path_planning import path_planning\n",
    "from src.vision import extend_polygons\n",
    "from src.utils import add_robot, add_route, add_circles, add_polygons, add_point, add_line\n",
    "from src.geometry import convert_point, convert_vec, define_line_p, define_line_v, dir_to_line, get_angle\n",
    "from src.navigation import change_target, FSM, global_control, global_control_path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============ROBOT INSTRUCTIONS==================================\n",
    "#function to get the value of the prox\n",
    "async def get_prox_value():\n",
    "    await node.wait_for_variables({\"prox.horizontal\"})\n",
    "    #await client.sleep(0.1)\n",
    "    return list(node[\"prox.horizontal\"])\n",
    "\n",
    "#set the speed of the motors\n",
    "async def set_motor_speed(left, right):\n",
    "    await node.set_variables({\"motor.left.target\": [left], \"motor.right.target\": [right]})\n",
    "\n",
    "await set_motor_speed(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_path = '../assets/additional/identity_calibration/'\n",
    "\n",
    "# https://www.geeksforgeeks.org/python-opencv-capture-video-from-camera/\n",
    "# Initialize the camera\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "# Get the default frame width and height\n",
    "frame_width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a picture\n",
    "ret, initial_image = cam.read()\n",
    "plt.imshow(cv2.cvtColor(initial_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[1, 2], [2, 2], [2, 1], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 3],\n",
       "       [2, 4],\n",
       "       [1, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_max = 5\n",
    "np.stack((points[:, 0], y_max - points[:, 1]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vision import VisionDetector\n",
    "from src.utils import add_robot\n",
    "\n",
    "aruco_info = json.load(open('../assets/aruco/state.json', 'r'))\n",
    "\n",
    "map_size = (1179, 830) # (width, height) in mm\n",
    "detector = VisionDetector(initial_image, map_size, calibration_path, undistort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the initial corrected image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_image = detector.get_corrected_image(initial_image)\n",
    "plt.imshow(cv2.cvtColor(corrected_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.geometry import define_line_p, convert_points\n",
    "xy = np.array([0,0])\n",
    "\n",
    "convert_points(xy, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidnapping_threshold = 100\n",
    "objectives_positions: List[List] = []\n",
    "\n",
    "async def main_loop(cam, speed):\n",
    "    camera_vis = True\n",
    "\n",
    "    # initializer kalman\n",
    "\n",
    "    while True:\n",
    "        previous_camera_vis = camera_vis\n",
    "        #get the prox sensors values\n",
    "        prox = await get_prox_value()\n",
    "\n",
    "        try:\n",
    "            ret, frame = cam.read()\n",
    "            corrected_image = detector.get_corrected_image(frame)\n",
    "            robot_pos, robot_orientation, robot_corners = detector.get_robot_pose(corrected_image, is_corrected=True)\n",
    "        except:\n",
    "            print('Camera is not available')\n",
    "            camera_vis = False\n",
    "        \n",
    "        fsm_state, leds_top, controller = FSM(fsm_state, leds_top, prox, path_m, robot_pos_m, robot_orientation_m)\n",
    "        motor_speed = [speed - int(controller), speed + int(controller)]\n",
    "        \n",
    "        current_position, current_orientation = kalman(..., camera_vis)\n",
    "\n",
    "        if camera_vis and not previous_camera_vis:\n",
    "            print('Camera is back')\n",
    "            if (np.linalg.norm(current_position - previous_position) > kidnapping_threshold):\n",
    "                print('Kidnapping detected')\n",
    "                print('Re-planning...')\n",
    "                #replan\n",
    "                # global_path_cv = replan_path(global_path_cv, robot_position, robot_orientation, corrected_image)\n",
    "                path_planning(robot_position, objectives_positions, extended_polygons, processed_image.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        previous_position = current_position\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bomr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
